# coding: utf-8

"""
    BlackFox

    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501

    OpenAPI spec version: v1
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""


import pprint
import re  # noqa: F401

import six


class KerasRecurrentHiddenLayerConfig(object):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """

    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'recurrent_activation_function': 'str',
        'recurrent_dropout': 'float',
        'neuron_count': 'int',
        'activation_function': 'str',
        'dropout': 'float'
    }

    attribute_map = {
        'recurrent_activation_function': 'recurrentActivationFunction',
        'recurrent_dropout': 'recurrentDropout',
        'neuron_count': 'neuronCount',
        'activation_function': 'activationFunction',
        'dropout': 'dropout'
    }

    def __init__(self, recurrent_activation_function=None, recurrent_dropout=None, neuron_count=None, activation_function=None, dropout=None):  # noqa: E501
        """KerasRecurrentHiddenLayerConfig - a model defined in Swagger"""  # noqa: E501

        self._recurrent_activation_function = None
        self._recurrent_dropout = None
        self._neuron_count = None
        self._activation_function = None
        self._dropout = None
        self.discriminator = None

        if recurrent_activation_function is not None:
            self.recurrent_activation_function = recurrent_activation_function
        if recurrent_dropout is not None:
            self.recurrent_dropout = recurrent_dropout
        if neuron_count is not None:
            self.neuron_count = neuron_count
        if activation_function is not None:
            self.activation_function = activation_function
        if dropout is not None:
            self.dropout = dropout

    @property
    def recurrent_activation_function(self):
        """Gets the recurrent_activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501


        :return: The recurrent_activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :rtype: str
        """
        return self._recurrent_activation_function

    @recurrent_activation_function.setter
    def recurrent_activation_function(self, recurrent_activation_function):
        """Sets the recurrent_activation_function of this KerasRecurrentHiddenLayerConfig.


        :param recurrent_activation_function: The recurrent_activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :type: str
        """
        allowed_values = ["SoftMax", "Elu", "Selu", "SoftPlus", "SoftSign", "ReLu", "TanH", "Sigmoid", "HardSigmoid", "Linear"]  # noqa: E501
        if recurrent_activation_function not in allowed_values:
            raise ValueError(
                "Invalid value for `recurrent_activation_function` ({0}), must be one of {1}"  # noqa: E501
                .format(recurrent_activation_function, allowed_values)
            )

        self._recurrent_activation_function = recurrent_activation_function

    @property
    def recurrent_dropout(self):
        """Gets the recurrent_dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501


        :return: The recurrent_dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :rtype: float
        """
        return self._recurrent_dropout

    @recurrent_dropout.setter
    def recurrent_dropout(self, recurrent_dropout):
        """Sets the recurrent_dropout of this KerasRecurrentHiddenLayerConfig.


        :param recurrent_dropout: The recurrent_dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :type: float
        """

        self._recurrent_dropout = recurrent_dropout

    @property
    def neuron_count(self):
        """Gets the neuron_count of this KerasRecurrentHiddenLayerConfig.  # noqa: E501


        :return: The neuron_count of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :rtype: int
        """
        return self._neuron_count

    @neuron_count.setter
    def neuron_count(self, neuron_count):
        """Sets the neuron_count of this KerasRecurrentHiddenLayerConfig.


        :param neuron_count: The neuron_count of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :type: int
        """

        self._neuron_count = neuron_count

    @property
    def activation_function(self):
        """Gets the activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501


        :return: The activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :rtype: str
        """
        return self._activation_function

    @activation_function.setter
    def activation_function(self, activation_function):
        """Sets the activation_function of this KerasRecurrentHiddenLayerConfig.


        :param activation_function: The activation_function of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :type: str
        """
        allowed_values = ["SoftMax", "Elu", "Selu", "SoftPlus", "SoftSign", "ReLu", "TanH", "Sigmoid", "HardSigmoid", "Linear"]  # noqa: E501
        if activation_function not in allowed_values:
            raise ValueError(
                "Invalid value for `activation_function` ({0}), must be one of {1}"  # noqa: E501
                .format(activation_function, allowed_values)
            )

        self._activation_function = activation_function

    @property
    def dropout(self):
        """Gets the dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501


        :return: The dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :rtype: float
        """
        return self._dropout

    @dropout.setter
    def dropout(self, dropout):
        """Sets the dropout of this KerasRecurrentHiddenLayerConfig.


        :param dropout: The dropout of this KerasRecurrentHiddenLayerConfig.  # noqa: E501
        :type: float
        """

        self._dropout = dropout

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, KerasRecurrentHiddenLayerConfig):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        return not self == other
